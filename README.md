# Proyecto cierre m贸dulo 3: An谩lisis de E-commerce
Repositorio dedicado al an谩lisis exploratorio, limpieza y transformaci贸n de datos de un entorno e-commerce utilizando Python.

## Descripci贸n del Proyecto
Este proyecto consiste en el desarrollo de un flujo de trabajo automatizado para la obtenci贸n, limpieza y estructuraci贸n de datos de una empresa de e-commerce. El objetivo es transformar datos crudos provenientes de m煤ltiples fuentes (CSV, Excel, Web) en un dataset confiable listo para modelos de Machine Learning y reportes estrat茅gicos.

##  Instrucciones para Ejecutar el Proyecto

Para reproducir este flujo de trabajo, ejecute los scripts en el siguiente orden:
- `clase1_numpy.py`: Generaci贸n y fundamentos de arrays.
- `clase2_panda.py`: Estructuras de datos (Series y DataFrames).
- `clase3_extraccion.py`: Lectura de fuentes externas (CSV, Excel, Web).
- `clase4_limpieza.py`: Tratamiento de nulos y Outliers (IQR).
- `clase5_wrangling.py`: Transformaci贸n avanzada y enriquecimiento.
- `clase6_reportes.py`: Agrupamiento y tablas din谩micas finales.

## Datos
Los datasets procesados se encuentran en la carpeta `/data`.

## Tecnolog铆as
- Python 3.13+
- Pandas / NumPy
- Openpyxl / Lxml

1. Justificaci贸n de Herramientas

NumPy: Se eligi贸 por su rapidez para crear y manejar grandes conjuntos de n煤meros, lo que facilita generar datos sint茅ticos (simulados) de forma eficiente.
Pandas: Se utiliz贸 para leer, limpiar y organizar los datos en tablas f谩ciles de trabajar. Permite cargar archivos (CSV, Excel, HTML) y transformar la informaci贸n para an谩lisis posteriores.

2. Descripci贸n de Datos y Fuentes

    Se cre贸 un conjunto de datos de ventas (DataSet) mediante simulaciones aleatorias para representar transacciones hist贸ricas.

    Fuentes externas
        - Archivo Excel: cat谩logo de categor铆as de productos que enriquece cada venta.
        - Extracci贸n de Indicadores: Uso de APIs (mindicador.cl) y Web Scraping para integrar el valor del d贸lar en tiempo real, contextualizando el an谩lisis financiero.

3. T茅cnicas de Limpieza y Transformaci贸n

    Manejo de Valores faltantes: Se aplic贸 identificaci贸n mediante isnull() e imputaci贸n con estad铆sticas de tendencia central (media/mediana para num茅ricos y moda para categ贸ricos).

    Outliers: Se identificaron y filtraron valores extremos usando el rango intercuartil (IQR), evitando que montos at铆picos afecten los an谩lisis.

    Data Wrangling:

        Binning: Segmentaci贸n o agrupaci贸n de ventas por niveles de gasto.

        Lambdas: C谩lculo de impuestos y totales de forma eficiente.

        Agregaci贸n: Resumen de m茅tricas mediante groupby() y pivot_table.

4. Decisiones y Desaf铆os

    Decisi贸n: Se prefiri贸 la mediana para imputar montos porque es menos sensible a valores extremos que la media.

    Desaf铆o: Resolver dependencias opcionales para la extracci贸n web (por ejemplo librer铆as adicionales) y asegurar que el proceso de carga fuera robusto. Manejo de Bloqueos Web: Se superaron errores de acceso (HTTP 403/429) mediante la implementaci贸n de planes de contingencia (datos de respaldo) para asegurar la continuidad del pipeline, se ten铆a presente usar User_agent tambi茅n.

    Integraci贸n: Al unir ventas con el cat谩logo se aplic贸 un left join para conservar todas las transacciones aunque falte informaci贸n del cat谩logo.

5. Resultados y Estado Final

    El dataset qued贸 normalizado, sin valores nulos y sin outliers extremos. Se exportaron reportes en Excel y CSV listos para usar en herramientas de visualizaci贸n o en modelos predictivos.

## Avance por Clases
### Clase 1: Cimentaci贸n con NumPy
- Generaci贸n de un conjunto de datos ficticios de clientes y transacciones.
- Implementaci贸n de operaciones estad铆sticas b谩sicas para an谩lisis preliminar.
- Exportaci贸n de datos en formato (`.npy`).

### Clase 2: Estructuraci贸n con Pandas
- Transformaci贸n de arreglos a **DataFrames**.
- Exploraci贸n de datos usando `.describe()`, `.info()` y `.value_counts()`.
- Aplicaci贸n de **filtros condicionales** para segmentar transacciones.
- Exportaci贸n a formato **CSV** para estandarizaci贸n de procesos.

### Clase 3: Extracci贸n Multi-fuente
- Implementaci贸n de `read_csv` con optimizaci贸n de tipos de datos (`dtype`).
- Manejo de archivos Excel mediante `read_excel` y  `openpyxl`.
- **Web Scraping** b谩sico: Consumo de APIs y manejo de peticiones mediante requests y urllib para capturar indicadores econ贸micos.
- Se aplic贸 `utf-8` y `latin1` para evitar errores de lectura

### Clase 4: Manejo de Valores Perdidos y Outliers
- Uso de `isnull().sum()` para dimensionar la falta de datos.
- Aplicaci贸n de la **Mediana** para montos mitigando el sesgo de valores extremos.
- Aplicaci贸n de la **Moda** para cantidad.
- Detecci贸n y filtrado de outliers con **Rango Intercuart铆lico (IQR)**.
- Reducci贸n de ruido en el dataset y creaci贸n de `dataset_limpio.csv`.

### Clase 5: Data Wrangling y Enriquecimiento
- Segmentaci贸n de transacciones en categor铆as de clientes por Segmento Etario (Joven, Adulto, S茅nior) y nivel de gasto (Bronce, Plata, Oro).
- Uso de funciones **Lambda**: Creaci贸n de un Score de Actividad normalizado (0-1) para identificar el compromiso del cliente.
- Renombramiento de columnas y reordenamiento estrat茅gico de filas para mejorar el reporte final.
- Conversi贸n de tipos de datos (`astype`) para asegurar la eficiencia en el procesamiento de grandes vol煤menes.

### Clase 6: Agrupamiento, Pivoteo e Integraci贸n Final
- Consolidaci贸n con `pd.merge()` con el cat谩logo de productos (leftjoin).
- Implementaci贸n de `groupby()` con funciones estad铆sticas (`agg`) para extraer m茅tricas clave.
- Creaci贸n de **Tablas Pivot** para an谩lisis cruzado de categor铆as de productos con segmentos de precio.
- Generaci贸n de reportes finales en formatos CSV y Excel para la toma de decisiones gerenciales.

---

## Conclusi贸n del Proyecto
Se ha implementado un flujo de datos (Pipeline) completo que:
1. **Obtiene** datos de fuentes heterog茅neas (NumPy, CSV, Excel, Web).
2. **Limpia** errores, nulos y outliers (IQR).
3. **Transforma** y enriquece la informaci贸n (Lambda, Binning).
4. **Analiza** y reporta resultados mediante agrupaciones complejas.

**El dataset final es confiable, estructurado y est谩 listo para ser consumido por modelos de Machine Learning o herramientas de visualizaci贸n.**
